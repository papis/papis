{
  "abstract": "PyTorch 2.0 stack support\n\nWe are very excited by the newly announced PyTorch 2.0 stack. You can enable `torch.compile` on any of our models, and get support with the `Trainer` (and in all our PyTorch examples) by using the `torchdynamo` training argument. For instance, just add `--torchdynamo inductor` when launching those examples from the command line.\n\nThis API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.\n\nNote that to get the best performance, we recommend:\n\n* using an Ampere GPU (or more recent)\n* sticking to fixed shaped for now (so use `--pad_to_max_length` in our examples)\n* Repurpose torchdynamo training args towards torch.\\_dynamo by @sgugger in #20498\n\nAudio Spectrogram Transformer\n\nThe Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.\n\n* Add Audio Spectogram Transformer by @NielsRogge in #19981\n\nJukebox\n\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.\n\n* Add Jukebox model (replaces #16875) by @ArthurZucker in #17826\n\nSwitch Transformers\n\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n\nIt is the first MoE model supported in `transformers`, with the largest checkpoint currently available currently containing 1T parameters.\n\n* Add Switch transformers by @younesbelkada and @ArthurZucker in #19323\n\nRocBert\n\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\n* Add RocBert by @sww9370 in #20013\n\nCLIPSeg\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo LÃ¼ddecke and Alexander Ecker. [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.\n\n* Add CLIPSeg by @NielsRogge in #20066\n\nNAT and DiNAT\nNAT\n\nNAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\nDiNAT\n\nDiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n\nIt extends [NAT](https://huggingface.co/docs/transformers/main/en/model_doc/nat) by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.\n\n* Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models by @alihassanijr in #20219\n\nMobileNetV2\n\nThe MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n\n* add MobileNetV2 model by @hollance in #17845\n\nMobileNetV1\n\nThe MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n\n* add MobileNetV1 model by @hollance in #17799\n\nImage processors\n\nImage processors replace feature extractors as the processing class for computer vision models.\n\nImportant changes:\n\n* `size` parameter is now a dictionary of `{\"height\": h, \"width\": w}`, `{\"shortest_edge\": s}`, `{\"shortest_egde\": s, \"longest_edge\": l}` instead of int or tuple.\n* Addition of `data_format` flag. You can now specify if you want your images to be returned in `\"channels_first\"` - NCHW - or `\"channels_last\"` - NHWC - format.\n* Processing flags e.g. `do_resize` can be passed directly to the `preprocess` method instead of modifying the class attribute: `image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")`\n* Leaving `return_tensors` unset will return a list of numpy arrays.\n\nThe classes are backwards compatible and can be created using existing feature extractor configurations - with the `size` parameter converted.\n\n* Add Image Processors by @amyeroberts in #19796\n* Add Donut image processor by @amyeroberts #20425\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* AutoImageProcessor by @amyeroberts in #20111\n\nBackbone for computer vision models\n\nWe're adding support for a general `AutoBackbone` class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.\n\n* Add AutoBackbone + ResNetBackbone by @NielsRogge in #20229\n* Improve backbone by @NielsRogge in #20380\n* [AutoBackbone] Improve API by @NielsRogge in #20407\n\nSupport for `safetensors` offloading\n\nIf the model you are using has a `safetensors` checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.\n\n* Safetensors offload by @sgugger in #20321\n\nContrastive search in the `generate` method\n\n* Generate: TF contrastive search with XLA support by @gante in #20050\n* Generate: contrastive search with full optional outputs by @gante in #19963\n\nBreaking changes\n\n* ðŸš¨ ðŸš¨ ðŸš¨ Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in `convert_tokens_to_string` by @beneyal in #15775\n\nBugfixes and improvements\n\n* add dataset by @stevhliu in #20005\n* Add BERT resources by @stevhliu in #19852\n* Add LayoutLMv3 resource by @stevhliu in #19932\n* fix typo by @stevhliu in #20006\n* Update object detection pipeline to use post\\_process\\_object\\_detection methods by @alaradirik in #20004\n* clean up vision/text config dict arguments by @ydshieh in #19954\n* make sentencepiece import conditional in bertjapanesetokenizer by @ripose-jp in #20012\n* Fix gradient checkpoint test in encoder-decoder by @ydshieh in #20017\n* Quality by @sgugger in #20002\n* Update auto processor to check image processor created by @amyeroberts in #20021\n* [Doctest] Add configuration\\_deberta\\_v2.py by @Saad135 in #19995\n* Improve model tester by @ydshieh in #19984\n* Fix doctest by @ydshieh in #20023\n* Show installed libraries and their versions in CI jobs by @ydshieh in #20026\n* reorganize glossary by @stevhliu in #20010\n* Now supporting pathlike in pipelines too. by @Narsil in #20030\n* Add \\*\\*kwargs by @amyeroberts in #20037\n* Fix some doctests after PR 15775 by @ydshieh in #20036\n* [Doctest] Add configuration\\_camembert.py by @Saad135 in #20039\n* [Whisper Tokenizer] Make more user-friendly by @sanchit-gandhi in #19921\n* [FuturWarning] Add futur warning for LEDForSequenceClassification by @ArthurZucker in #19066\n* fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc by @sywangyi in #19891\n* Update esmfold conversion script by @Rocketknight1 in #20028\n* Fixed torch.finfo issue with torch.fx by @michaelbenayoun in #20040\n* Only resize embeddings when necessary by @sgugger in #20043\n* Speed up TF token classification postprocessing by converting complete tensors to numpy by @deutschmn in #19976\n* Fix ESM LM head test by @Rocketknight1 in #20045\n* Update README.md by @bofenghuang in #20063\n* fix `tokenizer_type` to avoid error when loading checkpoint back by @pacman100 in #20062\n* [Trainer] Fix model name in push\\_to\\_hub by @sanchit-gandhi in #20064\n* PoolformerImageProcessor defaults to match previous FE by @amyeroberts in #20048\n* change constant torch.tensor to torch.full by @MerHS in #20061\n* Update READMEs for ESMFold and add notebooks by @Rocketknight1 in #20067\n* Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2 by @jordiclive in #20068\n* Allow passing arguments to model testers for CLIP-like models by @ydshieh in #20044\n* Show installed libraries and their versions in GA jobs by @ydshieh in #20069\n* Update defaults and logic to match old FE by @amyeroberts in #20065\n* Update modeling\\_tf\\_utils.py by @cakiki in #20076\n* Update hub.py by @cakiki in #20075\n* [Doctest] Add configuration\\_dpr.py by @Saad135 in #20080\n* Removing RobertaConfig inheritance from CamembertConfig by @Saad135 in #20059\n* Skip 2 tests in `VisionTextDualEncoderProcessorTest` by @ydshieh in #20098\n* Replace unsupported facebookresearch/bitsandbytes by @tomaarsen in #20093\n* docs: Resolve many typos in the English docs by @tomaarsen in #20088\n* use huggingface\\_hub.model\\_inifo() to get pipline\\_tag by @y-tag in #20077\n* Fix `generate_dummy_inputs` for `ImageGPTOnnxConfig` by @ydshieh in #20103\n* docs: Fixed variables in f-strings by @tomaarsen in #20087\n* Add new terms to the glossary by @stevhliu in #20051\n* Replace awkward timm link with the expected one by @tomaarsen in #20109\n* Fix AutoTokenizer with subfolder passed by @sgugger in #20110\n* [Audio Processor] Only pass sr to feat extractor by @sanchit-gandhi in #20022\n* Update github pr docs actions by @mishig25 in #20125\n* Adapt has\\_labels test when no labels were found by @sgugger in #20113\n* Improve tiny model creation script by @ydshieh in #20119\n* Remove BertConfig inheritance from RobertaConfig by @Saad135 in #20124\n* [Swin] Add Swin SimMIM checkpoints by @NielsRogge in #20034\n* Update `CLIPSegModelTester` by @ydshieh in #20134\n* Update SwinForMaskedImageModeling doctest values by @amyeroberts in #20139\n* Attempting to test automatically the `_keys_to_ignore`. by @Narsil in #20042\n* Generate: move generation\\_*.py src files into generation/*.py by @gante in #20096\n* add cv + audio labels by @stevhliu in #20114\n* Update VisionEncoderDecoder to use an image processor by @amyeroberts in #20137\n* [CLIPSeg] Add resources by @NielsRogge in #20118\n* Make DummyObject more robust by @mariosasko in #20146\n* Add `RoCBertTokenizer` to `TOKENIZER_MAPPING_NAMES` by @ydshieh in #20141\n* Adding support for LayoutLMvX variants for `object-detection`. by @Narsil in #20143\n* Add doc tests by @NielsRogge in #20158\n* doc comment fix: Args was in wrong place by @hollance in #20164\n* Update `OnnxConfig.generate_dummy_inputs` to check `ImageProcessingMixin` by @ydshieh in #20157\n* Generate: fix TF doctests by @gante in #20159\n* Fix arg names for our models by @Rocketknight1 in #20166\n* [processor] Add 'model input names' property by @sanchit-gandhi in #20117\n* Fix object-detection bug (height, width inversion). by @Narsil in #20167\n* [OWL-ViT] Make model consistent with CLIP by @NielsRogge in #20144\n* Fix type - update any PIL.Image.Resampling by @amyeroberts in #20172\n* Fix tapas scatter by @Bearnardd in #20149\n* Update README.md by @code-with-rajeev in #19530\n* Proposal Remove the weird `inspect` in ASR pipeline and make WhisperEncoder just nice to use. by @Narsil in #19571\n* Pytorch type hints by @IMvision12 in #20112\n* Generate: TF sample doctest result update by @gante in #20208\n* [ROC\\_BERT] Make CI happy by @younesbelkada in #20175\n* add \\_keys\\_to\\_ignore\\_on\\_load\\_unexpected = [r\"pooler\"] by @ArthurZucker in #20210\n* docs: translated index page to korean by @wonhyeongseo in #20180\n* feat: add i18n issue template by @wonhyeongseo in #20199\n* [Examples] Generalise Seq2Seq ASR to handle Whisper by @sanchit-gandhi in #19519\n* mark `test_save_load_fast_init_from_base` as `is_flaky` by @ydshieh in #20200\n* Update README.md by @Nietism in #20188\n* Downgrade log warning -> info by @amyeroberts in #20202\n* Generate: add Bloom fixes for contrastive search by @gante in #20213\n* Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm. by @Narsil in #20104\n* [docs] set overflowing image width to auto-scale by @wonhyeongseo in #20197\n* Update tokenizer\\_summary.mdx by @bofenghuang in #20135\n* Make `ImageSegmentationPipelineTests` less flaky by @ydshieh in #20147\n* update relative positional embedding by @ArthurZucker in #20203\n* [WHISPER] Update modeling tests by @ArthurZucker in #20162\n* Add `accelerate` support for `ViT` family by @younesbelkada in #20174\n* Add param\\_name to size\\_dict logs & tidy by @amyeroberts in #20205\n* Add object detection + segmentation transforms by @amyeroberts in #20003\n* Typo on doctring in ElectraTokenizer by @FacerAin in #20192\n* Remove `authorized_missing_keys`in favor of \\_keys\\_to\\_ignore\\_on\\_load\\_missing by @ArthurZucker in #20228\n* Add missing ESM autoclass by @Rocketknight1 in #20177\n* fix device issue by @ydshieh in #20227\n* fixed spelling error in testing.mdx by @kasmith11 in #20220\n* Fix `run_clip.py` by @ydshieh in #20234\n* Fix docstring of CLIPTokenizer(Fast) by @TilmannR in #20233\n* Fix MaskformerFeatureExtractor by @NielsRogge in #20100\n* New logging support to \"Trainer\" Class (ClearML Logger) by @skinan in #20184\n* Enable PyTorch 1.13 by @sgugger in #20168\n* [CLIP] allow loading projection layer in vision and text model by @patil-suraj in #18962\n* Slightly alter Keras dummy loss by @Rocketknight1 in #20232\n* Add to DeBERTa resources by @Saad135 in #20155\n* Add clip resources to the transformers documentation by @ambujpawar in #20190\n* Update reqs to include min gather\\_for\\_metrics Accelerate version by @muellerzr in #20242\n* Allow trainer to return eval. loss for CLIP-like models by @ydshieh in #20214\n* Adds image-guided object detection support to OWL-ViT by @alaradirik in #20136\n* Adding `audio-classification` example in the doc. by @Narsil in #20235\n* Updating the doctest for conversational. by @Narsil in #20236\n* Adding doctest for `fill-mask` pipeline. by @Narsil in #20241\n* Adding doctest for `feature-extraction`. by @Narsil in #20240\n* Adding ASR pipeline example. by @Narsil in #20226\n* Adding doctest for document-question-answering by @Narsil in #20239\n* Adding an example for `depth-estimation` pipeline. by @Narsil in #20237\n* Complete doc migration by @mishig25 in #20267\n* Fix result saving errors of pytorch examples by @li-plus in #20276\n* Adding a doctest for `table-question-answering` pipeline. by @Narsil in #20260\n* Adding doctest for `image-segmentation` pipeline. by @Narsil in #20256\n* Adding doctest for `text2text-generation` pipeline. by @Narsil in #20261\n* Adding doctest for `text-generation` pipeline. by @Narsil in #20264\n* Add TF protein notebook to notebooks doc by @Rocketknight1 in #20271\n* Rephrasing the link. by @Narsil in #20253\n* Add Chinese-CLIP implementation by @yangapku in #20368\n* Adding doctest example for `image-classification` pipeline. by @Narsil in #20254\n* Adding doctest for `zero-shot-image-classification` pipeline. by @Narsil in #20272\n* Adding doctest for `zero-shot-classification` pipeline. by @Narsil in #20268\n* Adding doctest for `visual-question-answering` pipeline. by @Narsil in #20266\n* Adding doctest for `text-classification` pipeline. by @Narsil in #20262\n* Adding doctest for `question-answering` pipeline. by @Narsil in #20259\n* [Docs] Add resources of OpenAI GPT by @shogohida in #20084\n* Adding doctest for `image-to-text` pipeline. by @Narsil in #20257\n* Adding doctest for `token-classification` pipeline. by @Narsil in #20265\n* remaining pytorch type hints by @IMvision12 in #20217\n* Data collator for token classification pads labels column when receives pytorch tensors by @markovalexander in #20244\n* [Doctest] Add configuration\\_deformable\\_detr.py by @Saad135 in #20273\n* Fix summarization script by @muellerzr in #20286\n* [DOCTEST] Fix the documentation of RoCBert by @ArthurZucker in #20142\n* [bnb] Let's warn users when saving 8-bit models by @younesbelkada in #20282\n* Adding `zero-shot-object-detection` pipeline doctest. by @Narsil in #20274\n* Adding doctest for `object-detection` pipeline. by @Narsil in #20258\n* Image transforms functionality used instead by @amyeroberts in #20278\n* TF: add test for `PushToHubCallback` by @gante in #20231\n* Generate: general TF XLA constrastive search are now slow tests by @gante in #20277\n* Fixing the doctests failures. by @Narsil in #20294\n* set the default cache\\_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast by @sywangyi in #20289\n* Add docstrings for canine model by @raghavanone in #19457\n* Add missing report button for Example test by @ydshieh in #20293\n* refactor test by @younesbelkada in #20300\n* [Tiny model creation] deal with `ImageProcessor` by @ydshieh in #20298\n* Fix blender bot missleading doc by @ArthurZucker in #20301\n* remove two tokens that should not be suppressed by @ArthurZucker in #20302\n* [ASR Examples] Update README for Whisper by @sanchit-gandhi in #20230\n* Add padding image transformation by @amyeroberts in #19838\n* Pin TensorFlow by @sgugger in #20313\n* Add AnyPrecisionAdamW optimizer by @atturaioe in #18961\n* [Proposal] Breaking change `zero-shot-object-detection` for improved consistency. by @Narsil in #20280\n* Fix flakey test with seed by @muellerzr in #20318\n* Pin TF 2.10.1 for Push CI by @ydshieh in #20319\n* Remove double brackets by @stevhliu in #20307\n* TF: future proof our keras imports by @gante in #20317\n* organize pipelines by modality by @stevhliu in #20306\n* Fix torch device issues by @ydshieh in #20304\n* Generate: add generation config class by @gante in #20218\n* translate zh quicktour by @bfss in #20095)\n* Add Spanish translation of serialization.mdx by @donelianc in #20245\n* Add LayerScale to NAT/DiNAT by @alihassanijr in #20325\n* [Switch Transformers] Fix failing slow test by @younesbelkada in #20346\n* fix: \"BigSicence\" typo in docs by @rajrajhans in #20331\n* Generate: `model_kwargs` can also be an input to `prepare_inputs_for_generation` by @gante in #20353\n* Update Special Language Tokens for PLBART by @jordiclive in #19980\n* Add resources by @NielsRogge in #20296\n* Enhance HfArgumentParser functionality and ease of use by @konstantinjdobler in #20323\n* Add inference section to task guides by @stevhliu in #18781\n* Fix toctree for Section 3 in Spanish Documentation by @donelianc in #20360\n* Generate: shorter XLA contrastive search tests by @gante in #20354\n* revert `keys_to_ignore` for M2M100 by @younesbelkada in #20381\n* add `accelerate` support for `ESM` by @younesbelkada in #20379\n* Fix nightly runs by @sgugger in #20352\n* Optimizes DonutProcessor token2json method for speed by @michaelnation26 in #20283\n* Indicate better minimal version of PyTorch in big model inference by @sgugger in #20385\n* Fix longformer onnx broken export by @fxmarty in #20292\n* Use tiny models for ONNX tests - text modality by @lewtun in #20333\n* [ESM] fix `accelerate` tests for esmfold by @younesbelkada in #20387\n* Generate: fix plbart generation tests by @gante in #20391\n* [bloom] convert script tweaks by @stas00 in #18593\n* Fix doctest file path by @ydshieh in #20400\n* [Image Transformers] to\\_pil fix float edge cases by @patrickvonplaten in #20406\n* make daily CI happy by @younesbelkada in #20410\n* fix nasty `bnb` bug by @younesbelkada in #20408\n* change the way sentinel tokens can retrived by @raghavanone in #20373\n* [BNB] Throw `ValueError` when trying to cast or assign by @younesbelkada in #20409\n* Use updated `model_max_length` when saving tokenizers by @ydshieh in #20401\n* Add Spanish translation of pr\\_checks.mdx by @donelianc in #20339\n* fix device in longformer onnx path by @fxmarty in #20419\n* Fix ModelOutput instantiation when there is only one tuple by @sgugger in #20416\n* `accelerate` support for `OwlViT` by @younesbelkada in #20411\n* [AnyPrecisionAdamW] test fix by @stas00 in #20454\n* fix `word_to_tokens` docstring format by @SaulLu in #20450\n* Fix typo in FSMT Tokenizer by @kamalkraj in #20456\n* Fix device issues in `CLIPSegModelIntegrationTest` by @ydshieh in #20467\n* Fix links for `contrastive_loss` by @ydshieh in #20455\n* Fix doctests for audio models by @ydshieh in #20468\n* Fix ESM checkpoints for tests by @Rocketknight1 in #20436\n* More TF int dtype fixes by @Rocketknight1 in #20384\n* make tensors in function build\\_relative\\_position created on proper device instead of always on cpu by @qq775294390 in #20434\n* update cpu related doc by @sywangyi in #20444\n* with pytorch cpu only version. without --no\\_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\" by @sywangyi in #20445\n* [CLIPTokenizer] Improve warning by @patrickvonplaten in #20458\n* Replace assertions with value errors on distilbert model by @JuheonChu in #20463\n* [Doctest] Add configuration\\_fsmt.py by @sha016 in #19936\n* Replace assertion with ValueError exceptions in run\\_image\\_captioning\\_flax.py by @katiele47 in #20365\n* [FLAX] Add dtype to embedding for bert/bart/opt/t5 by @merrymercy in #20340\n* fix both failing RoCBert tests by @ArthurZucker in #20469\n* Include image processor in add-new-model-like by @amyeroberts in #20439\n* chore: add link to the video cls notebook. by @sayakpaul in #20386\n* add timeout option for deepspeed engine by @henghuiz in #20443\n* [Maskformer] Add MaskFormerSwin backbone by @NielsRogge in #20344\n* Extract warnings from CI artifacts by @ydshieh in #20474\n* Add Donut image processor by @amyeroberts in #20425\n* Fix torch meshgrid warnings by @fxmarty in #20475\n* Fix init import\\_structure sorting by @sgugger in #20477\n* extract warnings in GH workflows by @ydshieh in #20487\n* add in layer gpt2 tokenizer by @piEsposito in #20421\n* Replace assert statements with raise exceptions by @miyu386 in #20478\n* fixed small typo by @sandeepgadhwal in #20490\n* Fix documentation code to import facebook/detr-resnet-50 model by @JuanFKurucz in #20491\n* Fix disk offload for full safetensors checkpoints by @sgugger in #20497\n* [modelcard] Check for IterableDataset by @sanchit-gandhi in #20495\n* [modelcard] Set model name if empty by @sanchit-gandhi in #20496\n* Add segmentation + object detection image processors by @amyeroberts in #20160\n* remove `attention_mask` truncation in whisper by @ydshieh in #20488\n* Make `add_special_tokens` more clear by @ydshieh in #20424\n* [OPT/Galactica] Load large `galactica` models by @younesbelkada in #20390\n* Support extraction of both train and eval XLA graphs by @jeffhataws in #20492\n* fix ipex+fp32 jit trace error in ipex 1.13 by @sywangyi in #20504\n* Expected output for the test changed by @ArthurZucker in #20493\n* Fix TF nightly tests by @Rocketknight1 in #20507\n* Update doc examples feature extractor -> image processor by @amyeroberts in #20501\n* Fix Typo in Docs for GPU by @julianpollmann in #20509\n* Fix minimum version for device\\_map by @sgugger in #20489\n* Update `AutomaticSpeechRecognitionPipeline` doc example by @ydshieh in #20512\n* Add `natten` for CI by @ydshieh in #20511\n* Fix Data2VecTextForCasualLM example code documentation by @JuanFKurucz in #20510\n* Add some warning for Dynamo and enable TF32 when it's set by @sgugger in #20515\n* [modelcard] Update dataset tags by @sanchit-gandhi in #20506\n* Change Doctests CI launch time by @ydshieh in #20523\n* Fix `PLBart` doctest by @ydshieh in #20527\n* Fix `ConditionalDetrForSegmentation` doc example by @ydshieh in #20531\n* add doc for by @younesbelkada in #20525\n* Update `ZeroShotObjectDetectionPipeline` doc example by @ydshieh in #20528\n* update post\\_process\\_image\\_guided\\_detection by @fcakyon in #20521\n* QnA example: add speed metric by @sywangyi in #20522\n* Fix doctest by @NielsRogge in #20534\n* Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code by @JuanFKurucz in #20516\n* Fix link in pipeline device map by @stevhliu in #20517\n\nSignificant community contributions\n\nThe following contributors have made significant changes to the library over the last release:\n\n* @sww9370\n  + Add RocBert (#20013)\n* @IMvision12\n  + Pytorch type hints (#20112)\n  + remaining pytorch type hints (#20217)\n* @alihassanijr\n  + Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)\n  + Add LayerScale to NAT/DiNAT (#20325)\n* @bfss\n  + translate zh quicktour(#20095) (#20181)\n* @donelianc\n  + Add Spanish translation of serialization.mdx (#20245)\n  + Fix toctree for Section 3 in Spanish Documentation (#20360)\n  + Add Spanish translation of pr\\_checks.mdx (#20339)\n* @yangapku\n  + Add Chinese-CLIP implementation (#20368)",
  "author": "Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.",
  "author_list": [
    {
      "family": "Wolf",
      "given": "Thomas"
    },
    {
      "family": "Debut",
      "given": "Lysandre"
    },
    {
      "family": "Sanh",
      "given": "Victor"
    },
    {
      "family": "Chaumond",
      "given": "Julien"
    },
    {
      "family": "Delangue",
      "given": "Clement"
    },
    {
      "family": "Moi",
      "given": "Anthony"
    },
    {
      "family": "Cistac",
      "given": "Perric"
    },
    {
      "family": "Ma",
      "given": "Clara"
    },
    {
      "family": "Jernite",
      "given": "Yacine"
    },
    {
      "family": "Plu",
      "given": "Julien"
    },
    {
      "family": "Xu",
      "given": "Canwen"
    },
    {
      "family": "Le Scao",
      "given": "Teven"
    },
    {
      "family": "Gugger",
      "given": "Sylvain"
    },
    {
      "family": "Drame",
      "given": "Mariama"
    },
    {
      "family": "Lhoest",
      "given": "Quentin"
    },
    {
      "family": "Rush",
      "given": "Alexander M."
    }
  ],
  "day": 1,
  "doi": "10.5281/zenodo.7391177",
  "eprint": 7391177,
  "license": "other-open",
  "month": 10,
  "note": "If you use this software, please cite it using these metadata.",
  "pubstate": "published",
  "revision": 3,
  "title": "Transformers: State-of-the-Art Natural Language Processing",
  "type": "software",
  "url": "https://zenodo.org/api/records/7391177",
  "version": "v4.25.1",
  "year": 2020
}