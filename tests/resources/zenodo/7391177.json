{"created": "2022-12-02T16:03:38.189470+00:00", "modified": "2022-12-03T02:26:43.995097+00:00", "id": 7391177, "conceptrecid": "3385997", "doi": "10.5281/zenodo.7391177", "conceptdoi": "10.5281/zenodo.3385997", "doi_url": "https://doi.org/10.5281/zenodo.7391177", "metadata": {"title": "Transformers: State-of-the-Art Natural Language Processing", "doi": "10.5281/zenodo.7391177", "publication_date": "2020-10-01", "description": "PyTorch 2.0 stack support\n<p>We are very excited by the newly announced PyTorch 2.0 stack. You can enable <code>torch.compile</code> on any of our models, and get support with the <code>Trainer</code> (and in all our PyTorch examples) by using the <code>torchdynamo</code> training argument. For instance, just add <code>--torchdynamo inductor</code> when launching those examples from the command line.</p>\n<p>This API is still experimental and may be subject to changes as the PyTorch 2.0 stack matures.</p>\n<p>Note that to get the best performance, we recommend:</p>\n<ul>\n<li>using an Ampere GPU (or more recent)</li>\n<li><p>sticking to fixed shaped for now (so use <code>--pad_to_max_length</code> in our examples)</p>\n</li>\n<li><p>Repurpose torchdynamo training args towards torch._dynamo  by @sgugger in #20498</p>\n</li>\n</ul>\nAudio Spectrogram Transformer\n<p>The Audio Spectrogram Transformer model was proposed in <a href=\"https://arxiv.org/abs/2104.01778\">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/vit\">Vision Transformer</a> to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification.</p>\n<ul>\n<li>Add Audio Spectogram Transformer  by @NielsRogge in #19981</li>\n</ul>\nJukebox\n<p>The Jukebox model was proposed in <a href=\"https://arxiv.org/pdf/2005.00341.pdf\">Jukebox: A generative model for music</a> by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on an artist, genres and lyrics.</p>\n<ul>\n<li>Add Jukebox model (replaces #16875)  by @ArthurZucker in #17826</li>\n</ul>\nSwitch Transformers\n<p>The SwitchTransformers model was proposed in <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> by William Fedus, Barret Zoph, Noam Shazeer.</p>\n<p>It is the first MoE model supported in <code>transformers</code>, with the largest checkpoint currently available currently containing 1T parameters.</p>\n<ul>\n<li>Add Switch transformers  by @younesbelkada and @ArthurZucker  in #19323</li>\n</ul>\nRocBert\n<p>The RoCBert model was proposed in <a href=\"https://aclanthology.org/2022.acl-long.65.pdf\">RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</a> by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou. It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.</p>\n<ul>\n<li>Add RocBert  by @sww9370 in #20013</li>\n</ul>\nCLIPSeg\n<p>The CLIPSeg model was proposed in <a href=\"https://arxiv.org/abs/2112.10003\">Image Segmentation Using Text and Image Prompts</a> by Timo L\u00fcddecke and Alexander Ecker. <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/clip\">CLIP</a>Seg adds a minimal decoder on top of a frozen CLIP model for zero- and one-shot image segmentation.</p>\n<ul>\n<li>Add CLIPSeg  by @NielsRogge in #20066</li>\n</ul>\nNAT and DiNAT\nNAT\n<p>NAT was proposed in <a href=\"https://arxiv.org/abs/2204.07143\">Neighborhood Attention Transformer</a> by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.</p>\n<p>It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.</p>\nDiNAT\n<p>DiNAT was proposed in <a href=\"https://arxiv.org/abs/2209.15001\">Dilated Neighborhood Attention Transformer</a> by Ali Hassani and Humphrey Shi.</p>\n<p>It extends <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/nat\">NAT</a> by adding a Dilated Neighborhood Attention pattern to capture global context, and shows significant performance improvements over it.</p>\n<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models  by @alihassanijr in #20219</li>\n</ul>\nMobileNetV2\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1801.04381\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.</p>\n<ul>\n<li>add MobileNetV2 model  by @hollance in #17845</li>\n</ul>\nMobileNetV1\n<p>The MobileNet model was proposed in <a href=\"https://arxiv.org/abs/1704.04861\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.</p>\n<ul>\n<li>add MobileNetV1 model  by @hollance in #17799</li>\n</ul>\nImage processors\n<p>Image processors replace feature extractors as the processing class for computer vision models.</p>\n<p>Important changes:</p>\n<ul>\n<li><code>size</code> parameter is now a dictionary of <code>{\"height\": h, \"width\": w}</code>, <code>{\"shortest_edge\": s}</code>, <code>{\"shortest_egde\": s, \"longest_edge\": l}</code> instead of int or tuple. </li>\n<li>Addition of <code>data_format</code> flag. You can now specify if you want your images to be returned in <code>\"channels_first\"</code> - NCHW - or <code>\"channels_last\"</code> - NHWC - format. </li>\n<li>Processing flags e.g. <code>do_resize</code> can be passed directly to the <code>preprocess</code> method instead of modifying the class attribute: <code>image_processor([image_1, image_2], do_resize=False, return_tensors=\"pt\", data_format=\"channels_last\")</code></li>\n<li>Leaving <code>return_tensors</code> unset will return a list of numpy arrays.</li>\n</ul>\n<p>The classes are backwards compatible and can be created using existing feature extractor configurations - with the <code>size</code> parameter converted.</p>\n<ul>\n<li>Add Image Processors  by @amyeroberts in #19796</li>\n<li>Add Donut image processor by @amyeroberts #20425 </li>\n<li>Add segmentation + object detection image processors by @amyeroberts in #20160 </li>\n<li>AutoImageProcessor  by @amyeroberts in #20111</li>\n</ul>\nBackbone for computer vision models\n<p>We're adding support for a general <code>AutoBackbone</code> class, which turns any vision model (like ConvNeXt, Swin Transformer) into a backbone to be used with frameworks like DETR and Mask R-CNN. The design is in early stages and we welcome feedback.</p>\n<ul>\n<li>Add AutoBackbone + ResNetBackbone  by @NielsRogge in #20229</li>\n<li>Improve backbone  by @NielsRogge in #20380</li>\n<li>[AutoBackbone] Improve API  by @NielsRogge in #20407</li>\n</ul>\nSupport for <code>safetensors</code> offloading\n<p>If the model you are using has a <code>safetensors</code> checkpoint and you have the library installed, offload to disk will take advantage of this to be more memory efficient and roughly 33% faster.</p>\n<ul>\n<li>Safetensors offload  by @sgugger in #20321</li>\n</ul>\nContrastive search in the <code>generate</code> method\n<ul>\n<li>Generate: TF contrastive search with XLA support  by @gante in #20050</li>\n<li>Generate: contrastive search with full optional outputs  by @gante in #19963</li>\n</ul>\nBreaking changes\n<ul>\n<li>\ud83d\udea8 \ud83d\udea8 \ud83d\udea8 Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in <code>convert_tokens_to_string</code>  by @beneyal in #15775</li>\n</ul>\nBugfixes and improvements\n<ul>\n<li>add dataset  by @stevhliu in #20005</li>\n<li>Add BERT resources  by @stevhliu in #19852</li>\n<li>Add LayoutLMv3 resource  by @stevhliu in #19932</li>\n<li>fix typo  by @stevhliu in #20006</li>\n<li>Update object detection pipeline to use post_process_object_detection methods by @alaradirik in #20004</li>\n<li>clean up vision/text config dict arguments  by @ydshieh in #19954</li>\n<li>make sentencepiece import conditional in bertjapanesetokenizer  by @ripose-jp in #20012</li>\n<li>Fix gradient checkpoint test in encoder-decoder  by @ydshieh in #20017</li>\n<li>Quality  by @sgugger in #20002</li>\n<li>Update auto processor to check image processor created  by @amyeroberts in #20021</li>\n<li>[Doctest] Add configuration_deberta_v2.py  by @Saad135 in #19995</li>\n<li>Improve model tester  by @ydshieh in #19984</li>\n<li>Fix doctest  by @ydshieh in #20023</li>\n<li>Show installed libraries and their versions in CI jobs  by @ydshieh in #20026</li>\n<li>reorganize glossary  by @stevhliu in #20010</li>\n<li>Now supporting pathlike in pipelines too.  by @Narsil in #20030</li>\n<li>Add **kwargs  by @amyeroberts in #20037</li>\n<li>Fix some doctests after PR 15775  by @ydshieh in #20036</li>\n<li>[Doctest] Add configuration_camembert.py  by @Saad135 in #20039</li>\n<li>[Whisper Tokenizer] Make more user-friendly  by @sanchit-gandhi in #19921</li>\n<li>[FuturWarning] Add futur warning for LEDForSequenceClassification  by @ArthurZucker in #19066</li>\n<li>fix jit trace error for model forward sequence is not aligned with jit.trace tuple input sequence, update related doc  by @sywangyi in #19891</li>\n<li>Update esmfold conversion script  by @Rocketknight1 in #20028</li>\n<li>Fixed torch.finfo issue with torch.fx  by @michaelbenayoun in #20040</li>\n<li>Only resize embeddings when necessary  by @sgugger in #20043</li>\n<li>Speed up TF token classification postprocessing by converting complete tensors to numpy  by @deutschmn in #19976</li>\n<li>Fix ESM LM head test  by @Rocketknight1 in #20045</li>\n<li>Update README.md  by @bofenghuang in #20063</li>\n<li>fix <code>tokenizer_type</code> to avoid error when loading checkpoint back  by @pacman100 in #20062</li>\n<li>[Trainer] Fix model name in push_to_hub  by @sanchit-gandhi in #20064</li>\n<li>PoolformerImageProcessor defaults to match previous FE  by @amyeroberts in #20048</li>\n<li>change constant torch.tensor to torch.full  by @MerHS in #20061</li>\n<li>Update READMEs for ESMFold and add notebooks  by @Rocketknight1 in #20067</li>\n<li>Update documentation on seq2seq models with absolute positional embeddings, to be in line with Tips section for BERT and GPT2  by @jordiclive in #20068</li>\n<li>Allow passing arguments to model testers for CLIP-like models  by @ydshieh in #20044</li>\n<li>Show installed libraries and their versions in GA jobs  by @ydshieh in #20069</li>\n<li>Update defaults and logic to match old FE  by @amyeroberts in #20065</li>\n<li>Update modeling_tf_utils.py  by @cakiki in #20076</li>\n<li>Update hub.py  by @cakiki in #20075</li>\n<li>[Doctest] Add configuration_dpr.py  by @Saad135 in #20080</li>\n<li>Removing RobertaConfig inheritance from CamembertConfig  by @Saad135 in #20059</li>\n<li>Skip 2 tests in <code>VisionTextDualEncoderProcessorTest</code>  by @ydshieh in #20098</li>\n<li>Replace unsupported facebookresearch/bitsandbytes  by @tomaarsen in #20093</li>\n<li>docs: Resolve many typos in the English docs  by @tomaarsen in #20088</li>\n<li>use huggingface_hub.model_inifo() to get pipline_tag  by @y-tag in #20077</li>\n<li>Fix <code>generate_dummy_inputs</code> for <code>ImageGPTOnnxConfig</code>  by @ydshieh in #20103</li>\n<li>docs: Fixed variables in f-strings  by @tomaarsen in #20087</li>\n<li>Add new terms to the glossary  by @stevhliu in #20051</li>\n<li>Replace awkward timm link with the expected one  by @tomaarsen in #20109</li>\n<li>Fix AutoTokenizer with subfolder passed  by @sgugger in #20110</li>\n<li>[Audio Processor] Only pass sr to feat extractor  by @sanchit-gandhi in #20022</li>\n<li>Update github pr docs actions  by @mishig25 in #20125</li>\n<li>Adapt has_labels test when no labels were found  by @sgugger in #20113</li>\n<li>Improve tiny model creation script  by @ydshieh in #20119</li>\n<li>Remove BertConfig inheritance from RobertaConfig  by @Saad135 in #20124</li>\n<li>[Swin] Add Swin SimMIM checkpoints  by @NielsRogge in #20034</li>\n<li>Update <code>CLIPSegModelTester</code>  by @ydshieh in #20134</li>\n<li>Update SwinForMaskedImageModeling doctest values  by @amyeroberts in #20139</li>\n<li>Attempting to test automatically the <code>_keys_to_ignore</code>.  by @Narsil in #20042</li>\n<li>Generate: move generation_<em>.py src files into generation/</em>.py  by @gante in #20096</li>\n<li>add cv + audio labels  by @stevhliu in #20114</li>\n<li>Update VisionEncoderDecoder to use an image processor  by @amyeroberts in #20137</li>\n<li>[CLIPSeg] Add resources  by @NielsRogge in #20118</li>\n<li>Make DummyObject more robust  by @mariosasko in #20146</li>\n<li>Add <code>RoCBertTokenizer</code> to <code>TOKENIZER_MAPPING_NAMES</code>  by @ydshieh in #20141</li>\n<li>Adding support for LayoutLMvX variants for <code>object-detection</code>.  by @Narsil in #20143</li>\n<li>Add doc tests  by @NielsRogge in #20158</li>\n<li>doc comment fix: Args was in wrong place  by @hollance in #20164</li>\n<li>Update <code>OnnxConfig.generate_dummy_inputs</code> to check <code>ImageProcessingMixin</code>  by @ydshieh in #20157</li>\n<li>Generate: fix TF doctests  by @gante in #20159</li>\n<li>Fix arg names for our models  by @Rocketknight1 in #20166</li>\n<li>[processor] Add 'model input names' property  by @sanchit-gandhi in #20117</li>\n<li>Fix object-detection bug (height, width inversion).  by @Narsil in #20167</li>\n<li>[OWL-ViT] Make model consistent with CLIP  by @NielsRogge in #20144</li>\n<li>Fix type - update any PIL.Image.Resampling  by @amyeroberts in #20172</li>\n<li>Fix tapas scatter  by @Bearnardd in #20149</li>\n<li>Update README.md  by @code-with-rajeev in #19530</li>\n<li>Proposal Remove the weird <code>inspect</code> in ASR pipeline and make WhisperEncoder just nice to use.  by @Narsil in #19571</li>\n<li>Pytorch type hints  by @IMvision12 in #20112</li>\n<li>Generate: TF sample doctest result update  by @gante in #20208</li>\n<li>[ROC_BERT] Make CI happy  by @younesbelkada in #20175</li>\n<li>add _keys_to_ignore_on_load_unexpected = [r\"pooler\"]  by @ArthurZucker in #20210</li>\n<li>docs: translated index page to korean  by @wonhyeongseo in #20180</li>\n<li>feat: add i18n issue template  by @wonhyeongseo in #20199</li>\n<li>[Examples] Generalise Seq2Seq ASR to handle Whisper  by @sanchit-gandhi in #19519</li>\n<li>mark <code>test_save_load_fast_init_from_base</code> as <code>is_flaky</code>  by @ydshieh in #20200</li>\n<li>Update README.md  by @Nietism in #20188</li>\n<li>Downgrade log warning -&gt; info  by @amyeroberts in #20202</li>\n<li>Generate: add Bloom fixes for contrastive search  by @gante in #20213</li>\n<li>Adding chunking for whisper (all seq2seq actually). Very crude matching algorithm.  by @Narsil in #20104</li>\n<li>[docs] set overflowing image width to auto-scale  by @wonhyeongseo in #20197</li>\n<li>Update tokenizer_summary.mdx  by @bofenghuang in #20135</li>\n<li>Make <code>ImageSegmentationPipelineTests</code> less flaky  by @ydshieh in #20147</li>\n<li>update relative positional embedding  by @ArthurZucker in #20203</li>\n<li>[WHISPER] Update modeling tests  by @ArthurZucker in #20162</li>\n<li>Add <code>accelerate</code> support for <code>ViT</code> family  by @younesbelkada in #20174</li>\n<li>Add param_name to size_dict logs &amp; tidy  by @amyeroberts in #20205</li>\n<li>Add object detection + segmentation transforms  by @amyeroberts in #20003</li>\n<li>Typo on doctring in ElectraTokenizer  by @FacerAin in #20192</li>\n<li>Remove <code>authorized_missing_keys</code>in favor of _keys_to_ignore_on_load_missing  by @ArthurZucker in #20228</li>\n<li>Add missing ESM autoclass  by @Rocketknight1 in #20177</li>\n<li>fix device issue  by @ydshieh in #20227</li>\n<li>fixed spelling error in testing.mdx  by @kasmith11 in #20220</li>\n<li>Fix <code>run_clip.py</code>  by @ydshieh in #20234</li>\n<li>Fix docstring of CLIPTokenizer(Fast)  by @TilmannR in #20233</li>\n<li>Fix MaskformerFeatureExtractor  by @NielsRogge in #20100</li>\n<li>New logging support to \"Trainer\" Class (ClearML Logger)  by @skinan in #20184</li>\n<li>Enable PyTorch 1.13  by @sgugger in #20168</li>\n<li>[CLIP] allow loading projection layer in vision and text model  by @patil-suraj in #18962</li>\n<li>Slightly alter Keras dummy loss  by @Rocketknight1 in #20232</li>\n<li>Add to DeBERTa resources  by @Saad135 in #20155</li>\n<li>Add clip resources to the transformers documentation  by @ambujpawar in #20190</li>\n<li>Update reqs to include min gather_for_metrics Accelerate version  by @muellerzr in #20242</li>\n<li>Allow trainer to return eval. loss for CLIP-like models  by @ydshieh in #20214</li>\n<li>Adds image-guided object detection support to OWL-ViT  by @alaradirik in #20136</li>\n<li>Adding <code>audio-classification</code> example in the doc.  by @Narsil in #20235</li>\n<li>Updating the doctest for conversational.  by @Narsil in #20236</li>\n<li>Adding doctest for <code>fill-mask</code> pipeline.  by @Narsil in #20241</li>\n<li>Adding doctest for <code>feature-extraction</code>.  by @Narsil in #20240</li>\n<li>Adding ASR pipeline example.  by @Narsil in #20226</li>\n<li>Adding doctest for document-question-answering  by @Narsil in #20239</li>\n<li>Adding an example for <code>depth-estimation</code> pipeline.  by @Narsil in #20237</li>\n<li>Complete doc migration  by @mishig25 in #20267</li>\n<li>Fix result saving errors of pytorch examples  by @li-plus in #20276</li>\n<li>Adding a doctest for <code>table-question-answering</code> pipeline.  by @Narsil in #20260</li>\n<li>Adding doctest for <code>image-segmentation</code> pipeline.  by @Narsil in #20256</li>\n<li>Adding doctest for <code>text2text-generation</code> pipeline.  by @Narsil in #20261</li>\n<li>Adding doctest for <code>text-generation</code> pipeline.  by @Narsil in #20264</li>\n<li>Add TF protein notebook to notebooks doc  by @Rocketknight1 in #20271</li>\n<li>Rephrasing the link.  by @Narsil in #20253</li>\n<li>Add Chinese-CLIP implementation  by @yangapku in #20368</li>\n<li>Adding doctest example for <code>image-classification</code> pipeline.  by @Narsil in #20254</li>\n<li>Adding doctest for <code>zero-shot-image-classification</code> pipeline.  by @Narsil in #20272</li>\n<li>Adding doctest for <code>zero-shot-classification</code> pipeline.  by @Narsil in #20268</li>\n<li>Adding doctest for <code>visual-question-answering</code> pipeline.  by @Narsil in #20266</li>\n<li>Adding doctest for <code>text-classification</code> pipeline.  by @Narsil in #20262</li>\n<li>Adding doctest for <code>question-answering</code> pipeline.  by @Narsil in #20259</li>\n<li>[Docs] Add resources of OpenAI GPT  by @shogohida in #20084</li>\n<li>Adding doctest for <code>image-to-text</code> pipeline.  by @Narsil in #20257</li>\n<li>Adding doctest for <code>token-classification</code> pipeline.  by @Narsil in #20265</li>\n<li>remaining pytorch type hints  by @IMvision12 in #20217</li>\n<li>Data collator for token classification pads labels column when receives pytorch tensors  by @markovalexander in #20244</li>\n<li>[Doctest] Add configuration_deformable_detr.py  by @Saad135 in #20273</li>\n<li>Fix summarization script  by @muellerzr in #20286</li>\n<li>[DOCTEST] Fix the documentation of RoCBert  by @ArthurZucker in #20142</li>\n<li>[bnb] Let's warn users when saving 8-bit models  by @younesbelkada in #20282</li>\n<li>Adding <code>zero-shot-object-detection</code> pipeline doctest.  by @Narsil in #20274</li>\n<li>Adding doctest for <code>object-detection</code> pipeline.  by @Narsil in #20258</li>\n<li>Image transforms functionality used instead  by @amyeroberts in #20278</li>\n<li>TF: add test for <code>PushToHubCallback</code>  by @gante in #20231</li>\n<li>Generate: general TF XLA constrastive search are now slow tests  by @gante in #20277</li>\n<li>Fixing the doctests failures.  by @Narsil in #20294</li>\n<li>set the default cache_enable to True, aligned with the default value in pytorch cpu/cuda amp autocast  by @sywangyi in #20289</li>\n<li>Add docstrings for canine model  by @raghavanone in #19457</li>\n<li>Add missing report button for Example test  by @ydshieh in #20293</li>\n<li>refactor test  by @younesbelkada in #20300</li>\n<li>[Tiny model creation] deal with <code>ImageProcessor</code>  by @ydshieh in #20298</li>\n<li>Fix blender bot missleading doc  by @ArthurZucker in #20301</li>\n<li>remove two tokens that should not be suppressed  by @ArthurZucker in #20302</li>\n<li>[ASR Examples] Update README for Whisper  by @sanchit-gandhi in #20230</li>\n<li>Add padding image transformation  by @amyeroberts in #19838</li>\n<li>Pin TensorFlow  by @sgugger in #20313</li>\n<li>Add AnyPrecisionAdamW optimizer  by @atturaioe in #18961</li>\n<li>[Proposal] Breaking change <code>zero-shot-object-detection</code> for improved     consistency.  by @Narsil in #20280</li>\n<li>Fix flakey test with seed  by @muellerzr in #20318</li>\n<li>Pin TF 2.10.1 for Push CI  by @ydshieh in #20319</li>\n<li>Remove double brackets  by @stevhliu in #20307</li>\n<li>TF: future proof our keras imports  by @gante in #20317</li>\n<li>organize pipelines by modality  by @stevhliu in #20306</li>\n<li>Fix torch device issues  by @ydshieh in #20304</li>\n<li>Generate: add generation config class  by @gante in #20218</li>\n<li>translate zh quicktour by @bfss in #20095) </li>\n<li>Add Spanish translation of serialization.mdx  by @donelianc in #20245</li>\n<li>Add LayerScale to NAT/DiNAT  by @alihassanijr in #20325</li>\n<li>[Switch Transformers] Fix failing slow test  by @younesbelkada in #20346</li>\n<li>fix: \"BigSicence\" typo in docs  by @rajrajhans in #20331</li>\n<li>Generate: <code>model_kwargs</code> can also be an input to <code>prepare_inputs_for_generation</code>  by @gante in #20353</li>\n<li>Update Special Language Tokens for PLBART  by @jordiclive in #19980</li>\n<li>Add resources  by @NielsRogge in #20296</li>\n<li>Enhance HfArgumentParser functionality and ease of use  by @konstantinjdobler in #20323</li>\n<li>Add inference section to task guides  by @stevhliu in #18781</li>\n<li>Fix toctree for Section 3 in Spanish Documentation  by @donelianc in #20360</li>\n<li>Generate: shorter XLA contrastive search tests  by @gante in #20354</li>\n<li>revert <code>keys_to_ignore</code> for M2M100  by @younesbelkada in #20381</li>\n<li>add <code>accelerate</code> support for <code>ESM</code>  by @younesbelkada in #20379</li>\n<li>Fix nightly runs  by @sgugger in #20352</li>\n<li>Optimizes DonutProcessor token2json method for speed  by @michaelnation26 in #20283</li>\n<li>Indicate better minimal version of PyTorch in big model inference  by @sgugger in #20385</li>\n<li>Fix longformer onnx broken export  by @fxmarty in #20292</li>\n<li>Use tiny models for ONNX tests - text modality  by @lewtun in #20333</li>\n<li>[ESM] fix <code>accelerate</code> tests for esmfold  by @younesbelkada in #20387</li>\n<li>Generate: fix plbart generation tests  by @gante in #20391</li>\n<li>[bloom] convert script tweaks  by @stas00 in #18593</li>\n<li>Fix doctest file path  by @ydshieh in #20400</li>\n<li>[Image Transformers] to_pil fix float edge cases  by @patrickvonplaten in #20406</li>\n<li>make daily CI happy  by @younesbelkada in #20410</li>\n<li>fix nasty <code>bnb</code> bug  by @younesbelkada in #20408</li>\n<li>change the way sentinel tokens can retrived  by @raghavanone in #20373</li>\n<li>[BNB] Throw <code>ValueError</code> when trying to cast or assign  by @younesbelkada in #20409</li>\n<li>Use updated <code>model_max_length</code> when saving tokenizers  by @ydshieh in #20401</li>\n<li>Add Spanish translation of pr_checks.mdx  by @donelianc in #20339</li>\n<li>fix device in longformer onnx path  by @fxmarty in #20419</li>\n<li>Fix ModelOutput instantiation when there is only one tuple  by @sgugger in #20416</li>\n<li><code>accelerate</code> support for <code>OwlViT</code>  by @younesbelkada in #20411</li>\n<li>[AnyPrecisionAdamW] test fix  by @stas00 in #20454</li>\n<li>fix <code>word_to_tokens</code> docstring format  by @SaulLu in #20450</li>\n<li>Fix typo in FSMT Tokenizer  by @kamalkraj in #20456</li>\n<li>Fix device issues in <code>CLIPSegModelIntegrationTest</code>  by @ydshieh in #20467</li>\n<li>Fix links for <code>contrastive_loss</code>  by @ydshieh in #20455</li>\n<li>Fix doctests for audio models  by @ydshieh in #20468</li>\n<li>Fix ESM checkpoints for tests  by @Rocketknight1 in #20436</li>\n<li>More TF int dtype fixes  by @Rocketknight1 in #20384</li>\n<li>make tensors in function build_relative_position created on proper device instead of always on cpu  by @qq775294390 in #20434</li>\n<li>update cpu related doc  by @sywangyi in #20444</li>\n<li>with pytorch cpu only version. without --no_cuda, using --bf16 will trigger error like \"Your setup doesn't support bf16/gpu. You need torch&gt;=1.10, using Ampere GPU with cuda&gt;=11.0\"  by @sywangyi in #20445</li>\n<li>[CLIPTokenizer] Improve warning  by @patrickvonplaten in #20458</li>\n<li>Replace assertions with value errors on distilbert model  by @JuheonChu in #20463</li>\n<li>[Doctest] Add configuration_fsmt.py  by @sha016 in #19936</li>\n<li>Replace assertion with ValueError exceptions in run_image_captioning_flax.py  by @katiele47 in #20365</li>\n<li>[FLAX] Add dtype to embedding for bert/bart/opt/t5  by @merrymercy in #20340</li>\n<li>fix both failing RoCBert tests  by @ArthurZucker in #20469</li>\n<li>Include image processor in add-new-model-like  by @amyeroberts in #20439</li>\n<li>chore: add link to the video cls notebook.  by @sayakpaul in #20386</li>\n<li>add timeout option for deepspeed engine  by @henghuiz in #20443</li>\n<li>[Maskformer] Add MaskFormerSwin backbone  by @NielsRogge in #20344</li>\n<li>Extract warnings from CI artifacts  by @ydshieh in #20474</li>\n<li>Add Donut image processor  by @amyeroberts in #20425</li>\n<li>Fix torch meshgrid warnings  by @fxmarty in #20475</li>\n<li>Fix init import_structure sorting  by @sgugger in #20477</li>\n<li>extract warnings in GH workflows  by @ydshieh in #20487</li>\n<li>add in layer gpt2 tokenizer  by @piEsposito in #20421</li>\n<li>Replace assert statements with raise exceptions  by @miyu386 in #20478</li>\n<li>fixed small typo  by @sandeepgadhwal in #20490</li>\n<li>Fix documentation code to import facebook/detr-resnet-50 model  by @JuanFKurucz in #20491</li>\n<li>Fix disk offload for full safetensors checkpoints  by @sgugger in #20497</li>\n<li>[modelcard] Check for IterableDataset  by @sanchit-gandhi in #20495</li>\n<li>[modelcard] Set model name if empty  by @sanchit-gandhi in #20496</li>\n<li>Add segmentation + object detection image processors  by @amyeroberts in #20160</li>\n<li>remove <code>attention_mask</code> truncation in whisper  by @ydshieh in #20488</li>\n<li>Make <code>add_special_tokens</code> more clear  by @ydshieh in #20424</li>\n<li>[OPT/Galactica] Load large <code>galactica</code> models  by @younesbelkada in #20390</li>\n<li>Support extraction of both train and eval XLA graphs  by @jeffhataws in #20492</li>\n<li>fix ipex+fp32 jit trace error in ipex 1.13  by @sywangyi in #20504</li>\n<li>Expected output for the test changed  by @ArthurZucker in #20493</li>\n<li>Fix TF nightly tests  by @Rocketknight1 in #20507</li>\n<li>Update doc examples feature extractor -&gt; image processor  by @amyeroberts in #20501</li>\n<li>Fix Typo in Docs for GPU  by @julianpollmann in #20509</li>\n<li>Fix minimum version for device_map  by @sgugger in #20489</li>\n<li>Update <code>AutomaticSpeechRecognitionPipeline</code> doc example  by @ydshieh in #20512</li>\n<li>Add <code>natten</code> for CI  by @ydshieh in #20511</li>\n<li>Fix Data2VecTextForCasualLM example code documentation  by @JuanFKurucz in #20510</li>\n<li>Add some warning for Dynamo and enable TF32 when it's set  by @sgugger in #20515</li>\n<li>[modelcard] Update dataset tags  by @sanchit-gandhi in #20506</li>\n<li>Change Doctests CI launch time  by @ydshieh in #20523</li>\n<li>Fix <code>PLBart</code> doctest  by @ydshieh in #20527</li>\n<li>Fix <code>ConditionalDetrForSegmentation</code> doc example  by @ydshieh in #20531</li>\n<li>add doc for  by @younesbelkada in #20525</li>\n<li>Update <code>ZeroShotObjectDetectionPipeline</code> doc example  by @ydshieh in #20528</li>\n<li>update post_process_image_guided_detection  by @fcakyon in #20521</li>\n<li>QnA example: add speed metric  by @sywangyi in #20522</li>\n<li>Fix doctest  by @NielsRogge in #20534</li>\n<li>Fix Hubert models in TFHubertModel and TFHubertForCTC documentation code  by @JuanFKurucz in #20516</li>\n<li>Fix link in pipeline device map  by @stevhliu in #20517</li>\n</ul>\nSignificant community contributions\n<p>The following contributors have made significant changes to the library over the last release:</p>\n<ul>\n<li>@sww9370<ul>\n<li>Add RocBert (#20013)</li>\n</ul>\n</li>\n<li>@IMvision12<ul>\n<li>Pytorch type hints (#20112)</li>\n<li>remaining pytorch type hints (#20217)</li>\n</ul>\n</li>\n<li>@alihassanijr<ul>\n<li>Add Neighborhood Attention Transformer (NAT) and Dilated NAT (DiNAT) models (#20219)</li>\n<li>Add LayerScale to NAT/DiNAT (#20325)</li>\n</ul>\n</li>\n<li>@bfss<ul>\n<li>translate zh quicktour(#20095) (#20181)</li>\n</ul>\n</li>\n<li>@donelianc<ul>\n<li>Add Spanish translation of serialization.mdx (#20245)</li>\n<li>Fix toctree for Section 3 in Spanish Documentation (#20360)</li>\n<li>Add Spanish translation of pr_checks.mdx (#20339)</li>\n</ul>\n</li>\n<li>@yangapku<ul>\n<li>Add Chinese-CLIP implementation (#20368)</li>\n</ul>\n</li>\n</ul>", "access_right": "open", "creators": [{"name": "Wolf, Thomas", "affiliation": null}, {"name": "Debut, Lysandre", "affiliation": null}, {"name": "Sanh, Victor", "affiliation": null}, {"name": "Chaumond, Julien", "affiliation": null}, {"name": "Delangue, Clement", "affiliation": null}, {"name": "Moi, Anthony", "affiliation": null}, {"name": "Cistac, Perric", "affiliation": null}, {"name": "Ma, Clara", "affiliation": null}, {"name": "Jernite, Yacine", "affiliation": null}, {"name": "Plu, Julien", "affiliation": null}, {"name": "Xu, Canwen", "affiliation": null}, {"name": "Le Scao, Teven", "affiliation": null}, {"name": "Gugger, Sylvain", "affiliation": null}, {"name": "Drame, Mariama", "affiliation": null}, {"name": "Lhoest, Quentin", "affiliation": null}, {"name": "Rush, Alexander M.", "affiliation": null}], "related_identifiers": [{"identifier": "https://github.com/huggingface/transformers/tree/v4.25.1", "relation": "isSupplementTo", "scheme": "url"}], "version": "v4.25.1", "resource_type": {"title": "Software", "type": "software"}, "license": {"id": "other-open"}, "relations": {"version": [{"index": 94, "is_last": true, "parent": {"pid_type": "recid", "pid_value": "3385997"}}]}, "notes": "If you use this software, please cite it using these metadata."}, "title": "Transformers: State-of-the-Art Natural Language Processing", "links": {"self": "https://zenodo.org/api/records/7391177", "self_html": "https://zenodo.org/records/7391177", "self_doi": "https://zenodo.org/doi/10.5281/zenodo.7391177", "doi": "https://doi.org/10.5281/zenodo.7391177", "parent": "https://zenodo.org/api/records/3385997", "parent_html": "https://zenodo.org/records/3385997", "parent_doi": "https://zenodo.org/doi/10.5281/zenodo.3385997", "self_iiif_manifest": "https://zenodo.org/api/iiif/record:7391177/manifest", "self_iiif_sequence": "https://zenodo.org/api/iiif/record:7391177/sequence/default", "files": "https://zenodo.org/api/records/7391177/files", "media_files": "https://zenodo.org/api/records/7391177/media-files", "archive": "https://zenodo.org/api/records/7391177/files-archive", "archive_media": "https://zenodo.org/api/records/7391177/media-files-archive", "latest": "https://zenodo.org/api/records/7391177/versions/latest", "latest_html": "https://zenodo.org/records/7391177/latest", "draft": "https://zenodo.org/api/records/7391177/draft", "versions": "https://zenodo.org/api/records/7391177/versions", "access_links": "https://zenodo.org/api/records/7391177/access/links", "access_users": "https://zenodo.org/api/records/7391177/access/users", "access_request": "https://zenodo.org/api/records/7391177/access/request", "access": "https://zenodo.org/api/records/7391177/access", "reserve_doi": "https://zenodo.org/api/records/7391177/draft/pids/doi", "communities": "https://zenodo.org/api/records/7391177/communities", "communities-suggestions": "https://zenodo.org/api/records/7391177/communities-suggestions", "requests": "https://zenodo.org/api/records/7391177/requests"}, "updated": "2022-12-03T02:26:43.995097+00:00", "recid": "7391177", "revision": 3, "files": [{"id": "56fcedb0-8d46-4121-8a2c-f2edf8ff120e", "key": "huggingface/transformers-v4.25.1.zip", "size": 14365317, "checksum": "md5:c8369dbbc1c61822d6069d2d7aa8d263", "links": {"self": "https://zenodo.org/api/records/7391177/files/huggingface/transformers-v4.25.1.zip/content"}}], "owners": [{"id": 75471}], "status": "published", "stats": {"downloads": 2178, "unique_downloads": 1997, "views": 75240, "unique_views": 68090, "version_downloads": 63, "version_unique_downloads": 55, "version_unique_views": 3592, "version_views": 4002}, "state": "done", "submitted": true}